{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Administrator\\anaconda3\\envs\\cv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "import warnings\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing import image\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from sklearn.metrics import average_precision_score, ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Crime',\n",
       " 1: 'Thriller',\n",
       " 2: 'Fantasy',\n",
       " 3: 'Horror',\n",
       " 4: 'Sci-Fi',\n",
       " 5: 'Comedy',\n",
       " 6: 'Documentary',\n",
       " 7: 'Adventure',\n",
       " 8: 'Film-Noir',\n",
       " 9: 'Animation',\n",
       " 10: 'Romance',\n",
       " 11: 'Drama',\n",
       " 12: 'Western',\n",
       " 13: 'Musical',\n",
       " 14: 'Action',\n",
       " 15: 'Mystery',\n",
       " 16: 'War',\n",
       " 17: \"Children's\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = [\"Crime\", \"Thriller\", \"Fantasy\", \"Horror\", \"Sci-Fi\", \"Comedy\", \"Documentary\", \"Adventure\", \"Film-Noir\", \"Animation\", \"Romance\", \"Drama\", \"Western\", \"Musical\", \"Action\", \"Mystery\", \"War\", \"Children\\'s\"]\n",
    "mapping = {}\n",
    "for i in range(len(genres)):\n",
    "    mapping[i] = genres[i]\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(checkpoint_path, use_ema=False, num_classes=1000):\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict_key = 'state_dict'\n",
    "        if isinstance(checkpoint, dict):\n",
    "            if use_ema and 'state_dict_ema' in checkpoint:\n",
    "                state_dict_key = 'state_dict_ema'\n",
    "        if state_dict_key and state_dict_key in checkpoint:\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in checkpoint[state_dict_key].items():\n",
    "                # strip `module.` prefix\n",
    "                name = k[7:] if k.startswith('module') else k\n",
    "                new_state_dict[name] = v\n",
    "            state_dict = new_state_dict\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "\n",
    "        return state_dict\n",
    "    else:\n",
    "#         _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n",
    "        raise FileNotFoundError()\n",
    "\n",
    "\n",
    "def load_for_transfer_learning(model, checkpoint_path, use_ema=False, strict=False, num_classes=1000):\n",
    "    state_dict = load_state_dict(checkpoint_path, use_ema, num_classes)\n",
    "    model.load_state_dict(state_dict, strict=strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adopt performer encoder for tokens-to-token\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.t2t_vit import *\n",
    "\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-Flan-T5-248M\")\n",
    "model_gen = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-Flan-T5-248M\")\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model1 = DistilBertForSequenceClassification .from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=18)\n",
    "model1.config.id2label = mapping\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"dduy193/plot-classification\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"dduy193/plot-classification\")\n",
    "model2.config.id2label = mapping\n",
    "\n",
    "model3 = T2t_vit_14(img_size=224, num_classes=12)\n",
    "model3.fc = torch.nn.Linear(2048, len(genres))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodal(torch.nn.Module):\n",
    "    def __init__(self, model1, model2, model3):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.fc1 = torch.nn.Linear(18, 18)\n",
    "        self.fc2 = torch.nn.Linear(18, 18)\n",
    "        self.fc3 = torch.nn.Linear(12, 18)\n",
    "\n",
    "    def forward(self, \n",
    "                title_input_ids, title_attention_mask,\n",
    "                plot_input_ids, plot_attention_mask,\n",
    "                image_input):\n",
    "        title_output = self.model1(title_input_ids, title_attention_mask)\n",
    "        plot_output = self.model2(plot_input_ids, plot_attention_mask)\n",
    "        image_output = self.model3(image_input)\n",
    "\n",
    "        title_output = self.fc1(title_output.logits)\n",
    "        plot_output = self.fc2(plot_output.logits)\n",
    "        image_output = self.fc3(image_output)\n",
    "        \n",
    "        output = torch.add(title_output, plot_output)\n",
    "        output = torch.add(output, image_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Multimodal(model1, model2, model3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multimodal(\n",
       "  (model1): DistilBertForSequenceClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): DistilBertSdpaAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (model2): DistilBertForSequenceClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): DistilBertSdpaAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (model3): T2T_ViT(\n",
       "    (tokens_to_token): T2T_module(\n",
       "      (soft_split0): Unfold(kernel_size=(7, 7), dilation=1, padding=(2, 2), stride=(4, 4))\n",
       "      (soft_split1): Unfold(kernel_size=(3, 3), dilation=1, padding=(1, 1), stride=(2, 2))\n",
       "      (soft_split2): Unfold(kernel_size=(3, 3), dilation=1, padding=(1, 1), stride=(2, 2))\n",
       "      (attention1): Token_performer(\n",
       "        (kqv): Linear(in_features=147, out_features=192, bias=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (attention2): Token_performer(\n",
       "        (kqv): Linear(in_features=576, out_features=192, bias=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (project): Linear(in_features=576, out_features=384, bias=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-13): 14 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1152, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=384, out_features=12, bias=True)\n",
       "    (fc): Linear(in_features=2048, out_features=18, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=18, out_features=18, bias=True)\n",
       "  (fc2): Linear(in_features=18, out_features=18, bias=True)\n",
       "  (fc3): Linear(in_features=12, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('partially_frozen_multimodel.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(title: str, model: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer, device) -> str:\n",
    "    quote = 'What is the story of the movie {}?'\n",
    "    model_gen.to(device)\n",
    "    model_gen.eval()\n",
    "\n",
    "    input_ids = tokenizer(quote.format(title), return_tensors='pt').input_ids.to(device)\n",
    "    output = model.generate(input_ids, max_length=256, do_sample=True, temperature=0.09)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(title, image, \n",
    "              tokenizer1=tokenizer1, tokenizer2=tokenizer2, tokenizer_gen=tokenizer_gen,\n",
    "              model_gen=model_gen, model=model, \n",
    "              genres=genres, device=device):\n",
    "    title_input = tokenizer1(title, return_tensors='pt', padding=True, truncation=True)\n",
    "    title_input_ids = title_input['input_ids'].to(device)\n",
    "    title_attention_mask = title_input['attention_mask'].to(device)\n",
    "\n",
    "    plot = generate_plot(title, model_gen, tokenizer_gen, device)\n",
    "    plot_input = tokenizer2(plot, return_tensors='pt', padding=True, truncation=True)\n",
    "    plot_input_ids = plot_input['input_ids'].to(device)\n",
    "    plot_attention_mask = plot_input['attention_mask'].to(device)\n",
    "\n",
    "    # If image is not uploaded\n",
    "    if image is None:\n",
    "        image_input = torch.zeros((1, 3, 224, 224)).to(device)\n",
    "\n",
    "    else:\n",
    "        image_input = np.resize(image, (224, 224, image.shape[-1]))\n",
    "        image_input = v2.ToTensor()(image_input)\n",
    "        image_input = image_input.unsqueeze(0)\n",
    "        image_input = image_input.to(device)\n",
    "\n",
    "    output = model(title_input_ids, title_attention_mask, plot_input_ids, plot_attention_mask, image_input)\n",
    "    output = torch.sigmoid(output)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    output = np.where(output > 0.5, 1, 0)\n",
    "    output = output.squeeze()\n",
    "    output = np.where(output == 1)[0]\n",
    "    output = [genres[i] for i in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = gr.Interface(fn=inference, inputs=[\"text\", \"image\"], outputs=\"text\", title=\"Movie Genre Classification\", \n",
    "                   description=\"This model classifies the genre of a movie based on its title and poster.\", \n",
    "                   examples=[[\"The Matrix\", \"https://upload.wikimedia.org/wikipedia/en/c/c1/The_Matrix_Poster.jpg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://2aa25735dcfb4e856b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2aa25735dcfb4e856b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
